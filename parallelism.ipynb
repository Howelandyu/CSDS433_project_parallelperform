{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lY5k31Y0yZya"
   },
   "outputs": [],
   "source": [
    "from torch import nn, Tensor, randint, rand\n",
    "import torch\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_1_in, layer_1_out, layer_2_out):\n",
    "    super().__init__()\n",
    "    self.w1 = nn.Linear(layer_1_in,layer_1_out)\n",
    "    self.w2 = nn.Linear(layer_1_out,layer_2_out)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.w1(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    x = self.w2(x)\n",
    "    x = nn.Sigmoid()(x)\n",
    "    return x\n",
    "\n",
    "layer_1_in = 16\n",
    "layer_1_out = 8\n",
    "layer_2_in = layer_1_out\n",
    "layer_2_out = 1\n",
    "torch.manual_seed(40)\n",
    "batch_size = 32\n",
    "dataset_size = 320\n",
    "lr = .5\n",
    "x = rand([dataset_size, 16])\n",
    "target = randint(0,2, [dataset_size,1], dtype=torch.float32)\n",
    "model = MLP(layer_1_in, layer_1_out, layer_2_out)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rhtJCGbtXrZ"
   },
   "source": [
    "And a simple training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tke1Tsoa6jVm",
    "outputId": "0544fc2f-c189-4814-c783-ad27d949d98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0: 0.715\n",
      "loss at iteration 2000: 0.415\n",
      "loss at iteration 4000: 0.297\n",
      "loss at iteration 6000: 0.238\n",
      "loss at iteration 8000: 0.223\n",
      "loss at iteration 10000: 0.199\n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "  start = i*batch_size % dataset_size\n",
    "  end = (i+1)*batch_size % dataset_size\n",
    "  batch_x = x[start:end,:]\n",
    "  batch_target = target[start:end,:]\n",
    "  y = model(batch_x)\n",
    "  loss = nn.BCELoss()\n",
    "  output = loss(y, batch_target)\n",
    "  model.zero_grad()\n",
    "  output.backward()\n",
    "  optimizer.step()\n",
    "  if i%2000==0:\n",
    "    print(f\"loss at iteration {i}: {output.data:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKPBv4Ay2nRY"
   },
   "source": [
    "Training works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDZ1J9VZ6toM",
    "outputId": "ad73b828-55df-45fb-c17e-a3abae9d67b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0: 0.706\n",
      "loss at iteration 1000: 0.467\n",
      "loss at iteration 2000: 0.361\n",
      "loss at iteration 3000: 0.347\n",
      "loss at iteration 4000: 0.320\n",
      "loss at iteration 5000: 0.293\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "# redefine model copies for each node\n",
    "model = MLP(layer_1_in, layer_1_out, layer_2_out)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "for i in range(5001):\n",
    "  # copy model state to each node\n",
    "  model_0 = copy.deepcopy(model)\n",
    "  model_1 = copy.deepcopy(model)\n",
    "  \n",
    "  # Prepare a batch for each node\n",
    "  start_0 = i*batch_size % dataset_size\n",
    "  end_0 = (i+1)*batch_size % dataset_size\n",
    "  start_1 = (i+1)*batch_size % dataset_size\n",
    "  end_1 = (i+2)*batch_size % dataset_size\n",
    "  batch_x_0 = x[start_0:end_0,:]\n",
    "  batch_target_0 = target[start_0:end_0,:]\n",
    "  batch_x_1 = x[start_1:end_1,:]\n",
    "  batch_target_1 = target[start_1:end_1,:]\n",
    "\n",
    "  # reset grads\n",
    "  grads = defaultdict(list)\n",
    "\n",
    "  # compute grads on node 0\n",
    "  y_0 = model_0(batch_x_0)\n",
    "  output_0 = loss(y_0, batch_target_0)\n",
    "  model_0.zero_grad()\n",
    "  output_0.backward()\n",
    "  grads['w1_w'].append(model_0.w1.weight.grad)\n",
    "  grads['w1_b'].append(model_0.w1.bias.grad)\n",
    "  grads['w2_w'].append(model_0.w2.weight.grad)\n",
    "  grads['w2_b'].append(model_0.w2.bias.grad)\n",
    "\n",
    "  # compute grads on node 1\n",
    "  y_1 = model_1(batch_x_1)\n",
    "  output_1 = loss(y_1, batch_target_1)\n",
    "  model_1.zero_grad()\n",
    "  output_1.backward()\n",
    "  grads['w1_w'].append(model_1.w1.weight.grad)\n",
    "  grads['w1_b'].append(model_1.w1.bias.grad)\n",
    "  grads['w2_w'].append(model_1.w2.weight.grad)\n",
    "  grads['w2_b'].append(model_1.w2.bias.grad)\n",
    "\n",
    "  # accumulate the gradients from the nodes and update the main model\n",
    "  model.zero_grad()\n",
    "  model.w1.weight.grad = sum(grads['w1_w'])\n",
    "  model.w1.bias.grad = sum(grads['w1_b'])\n",
    "  model.w2.weight.grad = sum(grads['w2_w'])\n",
    "  model.w2.bias.grad = sum(grads['w2_b'])\n",
    "  optimizer.step() \n",
    "  \n",
    "  if i%1000==0:\n",
    "    # print the average loss since batch sizes vary\n",
    "    print(f\"loss at iteration {i}: {(output_0.data + output_1.data) / 2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye-rL-zj2sEH"
   },
   "source": [
    "Great! Training still works, and it only takes half the amount of iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKfCzzP38__H",
    "outputId": "99b7623e-c937-474f-b57c-49f942352f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0: 0.693\n",
      "loss at iteration 1000: 0.510\n",
      "loss at iteration 2000: 0.367\n",
      "loss at iteration 3000: 0.315\n",
      "loss at iteration 4000: 0.298\n",
      "loss at iteration 5000: 0.285\n"
     ]
    }
   ],
   "source": [
    "model = MLP(layer_1_in, layer_1_out, layer_2_out)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# distribute weights on nodes\n",
    "\n",
    "# Layer 1\n",
    "# we split the first layer along its output dimension\n",
    "w1_node_0 = nn.Linear(layer_1_in, layer_1_out//2)\n",
    "with torch.no_grad():\n",
    "  w1_node_0.weight.copy_(model.w1.weight[:4,:])\n",
    "  w1_node_0.bias.copy_(model.w1.bias[:4])\n",
    "\n",
    "w1_node_1 = nn.Linear(layer_1_in, layer_1_out//2)\n",
    "with torch.no_grad():\n",
    "  w1_node_1.weight.copy_(model.w1.weight[4:,:])\n",
    "  w1_node_1.bias.copy_(model.w1.bias[4:])\n",
    "\n",
    "# Layer 2\n",
    "# since the output of layer 2 is dimension 1, we split along the input dimension\n",
    "w2_node_0 = nn.Linear(layer_2_in//2, layer_2_out)\n",
    "with torch.no_grad():\n",
    "  w2_node_0.weight.copy_(model.w2.weight[:,:4])\n",
    "  w2_node_0.bias.copy_(model.w2.bias)\n",
    "\n",
    "# since there is only one bias, we put it arbitrarily on node 0 so bias=False on node 1\n",
    "# w2_node_1 = nn.Linear(*reversed(model.w2.weight[:,4:].shape), bias=False)\n",
    "w2_node_1 = nn.Linear(layer_2_in//2, layer_2_out, bias=False)\n",
    "with torch.no_grad():\n",
    "  w2_node_1.weight.copy_(model.w2.weight[:,4:])\n",
    "\n",
    "weights = [w1_node_0, w1_node_1, w2_node_0, w2_node_1]\n",
    "# we'll have twice the effective batch size, so we double the lr\n",
    "optimizers = [torch.optim.SGD(w.parameters(), lr=2*lr) for w in weights]\n",
    "\n",
    "# since we split the model in half, we can double the batch size (ignoring activations for simplicity)\n",
    "split_model_batch_size = 2 * batch_size\n",
    "for i in range(5001):\n",
    "  start = i*split_model_batch_size % dataset_size\n",
    "  end = (i+1)*split_model_batch_size % dataset_size\n",
    "  \n",
    "  batch_x = x[start:end,:]\n",
    "  batch_target = target[start:end,:]\n",
    "\n",
    "  # layer 1 forward pass\n",
    "  # node 0\n",
    "  h1_node_0 = nn.ReLU()(w1_node_0(batch_x))\n",
    "  # node 1\n",
    "  h1_node_1 = nn.ReLU()(w1_node_1(batch_x))\n",
    "  # gather activations to dispatch to both nodes\n",
    "  h1 = torch.cat([h1_node_0,h1_node_1], axis=1)\n",
    "\n",
    "  # layer 2 forward pass\n",
    "  # node 0\n",
    "  partial_sum_node_0 = w2_node_0(h1[:,:4])\n",
    "  # node 1\n",
    "  partial_sum_node_1 = w2_node_1(h1[:,4:])\n",
    "  # gather output\n",
    "  y = nn.Sigmoid()(partial_sum_node_0+partial_sum_node_1)\n",
    "  output = loss(y, batch_target)\n",
    "\n",
    "  # compute grads\n",
    "  # we simplify here, in reality the backward pass would be distributed similarly to the forward pass\n",
    "  # the logic is similar so we'll skip for readability. For example for layer 1:\n",
    "  # if h = x . W^T and we have dh, dW^T = x^T . dh, so dW = dh^T . x\n",
    "  # so we can get dW_1 by computing dh^T . x[:,:4] on node 1.\n",
    "  for w in weights:\n",
    "    w.zero_grad()\n",
    "  output.backward()\n",
    "  for o in optimizers:\n",
    "    o.step()\n",
    "    \n",
    "  if i%1000==0:\n",
    "    print(f\"loss at iteration {i}: {output.data:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7hgAlZV20d_"
   },
   "source": [
    "Once again, the loss is decreasing, and we are only going through half the iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgWj4rzkju8x",
    "outputId": "482839d8-36bf-489f-b135-f4c753735e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pipeline to start, batch 0\n",
      "Filling pipeline to start, batch 1\n",
      "loss at iteration 3: 0.691\n",
      "loss at iteration 1003: 0.599\n",
      "loss at iteration 2003: 0.423\n",
      "loss at iteration 3003: 0.367\n",
      "loss at iteration 4003: 0.348\n",
      "loss at iteration 5003: 0.330\n",
      "loss at iteration 6003: 0.426\n",
      "loss at iteration 7003: 0.334\n",
      "loss at iteration 8003: 0.276\n",
      "loss at iteration 9003: 0.280\n"
     ]
    }
   ],
   "source": [
    "# distribute weights on nodes\n",
    "\n",
    "# node 0 is the first stage\n",
    "class Stage0(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.w1 = nn.Linear(16,8)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.w1(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# node 1 is the second stage\n",
    "class Stage1(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.w2 = nn.Linear(8,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.w2(x)\n",
    "    x = nn.Sigmoid()(x)\n",
    "    return x\n",
    "\n",
    "stages = [Stage0(), Stage1()]\n",
    "optimizers = [torch.optim.SGD(s.parameters(), lr=lr) for s in stages]\n",
    "\n",
    "stage_0_backward_queue = []\n",
    "stage_1_forward_queue = []\n",
    "stage_1_backward_queue = []\n",
    "\n",
    "# keep track of whether the next step should be forward\n",
    "forward_stage = [True, False]\n",
    "\n",
    "# each nodes alternates between forwards and backwards pass\n",
    "batch_idx = 0\n",
    "while batch_idx < (10000):\n",
    "  # node 0\n",
    "  if forward_stage[0]:\n",
    "    # add new batch to batch list\n",
    "    start = batch_idx*batch_size % dataset_size\n",
    "    end = (batch_idx+1)*batch_size % dataset_size\n",
    "    if end==0:\n",
    "      end = -1\n",
    "    batch_x = x[start:end,:]\n",
    "    batch_target = target[start:end,:]\n",
    "\n",
    "    # forward pass and add to stage 1 queue, along with targets for loss computation\n",
    "    stage_1_forward_queue.append((stages[0](batch_x),batch_target))\n",
    "    \n",
    "    if batch_idx%1000==3:\n",
    "      print(f\"loss at iteration {batch_idx}: {batch_loss.data:.3f}\")\n",
    "    batch_idx += 1\n",
    "\n",
    "  else:\n",
    "    # we skip this block if stage 1 hasn't sent gradients back yet\n",
    "    if len(stage_0_backward_queue)>0:\n",
    "      # grab the gradients from stage 1, and propagate them and update parameters\n",
    "      output_with_grads = stage_0_backward_queue.pop(0)\n",
    "      stages[0].zero_grad()\n",
    "      output_with_grads.backward(gradient=output_with_grads.grad)\n",
    "      optimizers[0].step()\n",
    "  \n",
    "  # node 1\n",
    "  if forward_stage[1]:\n",
    "    # grab output from stage 0 along with target, pass forward and store in queue for next backwards pass\n",
    "    # we also store stage 0's output so we can pass it back along with its gradients after we backprop\n",
    "    stage_0_output, batch_target = stage_1_forward_queue.pop(0)\n",
    "    stage_1_backward_queue.append((stages[1](stage_0_output), stage_0_output, batch_target))\n",
    "  else:\n",
    "    # we check that we started the process and received gradients    \n",
    "    if len(stage_1_backward_queue)>0:\n",
    "      \n",
    "      stages[1].zero_grad()\n",
    "      stage_1_output, stage_0_output, batch_target = stage_1_backward_queue.pop(0)\n",
    "      batch_loss = loss(stage_1_output, batch_target)\n",
    "\n",
    "      # only backprop in the top stage variables since that is all we have on this node\n",
    "      # we also want the grad for output so we can pass it to stage 0\n",
    "      batch_loss.backward(inputs=[stages[1].w2.weight, stages[1].w2.bias, stage_0_output], retain_graph=True)\n",
    "      stage_0_backward_queue.append(stage_0_output)\n",
    "      optimizers[1].step()\n",
    "  \n",
    "  \n",
    "  # at first, we have nothing to backprop, so we keep passing examples forward in stage 0\n",
    "  if batch_idx>2:\n",
    "    forward_stage[0] = not forward_stage[0]\n",
    "  else:\n",
    "    print(f\"Filling pipeline to start, batch {batch_idx-1}\")\n",
    "  forward_stage[1] = not forward_stage[1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
